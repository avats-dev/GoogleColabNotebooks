{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"19_seq2seq.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/13_rnn/04_Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"G2BhrFGqFx_M"},"source":["# エンコーダ・デコーダによる計算機作成\n","\n","リカレントニューラルネットワークは，系列データ内の関連性を内部状態として保持することができます．\n","この内部状態を利用して，新たな出力ができるようにした構造としてエンコーダ・デコーダがあります．\n","エンコーダ側に系列データを入力して，中間層では系列データ内の関連性を内部状態を形成します．\n","デコーダ側には内部状態を与えることで，内部状態を反映した何かしらの結果を出力します．\n","この応用が，google 翻訳などの機械翻訳です．\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1zFl4Mjo4IRSQWSczJ4PzPkd53YJkb1oM\" width = 100%>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VCmM6-6zF2hG"},"source":["##計算機の実装\n","ここでは，エンコーダ・デコーダ構造で計算機（足し算）を作ってみます．\n","このエンコーダ・デコーダ構造のことをSeq2seqと呼びます．"]},{"cell_type":"markdown","metadata":{"id":"PvQCLiV4F1T6"},"source":["###データローダの作成\n","まず，データローダを用意します．データは0から9までの数字と加算記号，開始，終了のフラグです．また，３桁の数字の足し算を行うため，各桁の値を１つずつランダムに生成して連結しています．\n"]},{"cell_type":"code","metadata":{"id":"5wUc1QN0Fygb"},"source":["import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","word2id = {str(i): i for i in range(10)}\n","word2id.update({\"<pad>\": 10, \"+\": 11, \"<eos>\": 12})\n","id2word = {v: k for k, v in word2id.items()}\n","\n","class CalcDataset(torch.utils.data.Dataset):\n","\n","    def transform(self, string, seq_len=7):\n","        tmp = []\n","        for i, c in enumerate(string):\n","            try:\n","                tmp.append(word2id[c])\n","            except:\n","                tmp += [word2id[\"<pad>\"]] * (seq_len - i)\n","                break\n","        return tmp\n","\n","    def __init__(self, data_num, train=True):\n","        super().__init__()\n","        self.data_num = data_num\n","        self.train = train\n","        self.data = []\n","        self.label = []\n","\n","        for _ in range(data_num):\n","            x = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n","            y = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n","            left = (\"{:*<7s}\".format(str(x) + \"+\" + str(y))).replace(\"*\", \"<pad>\")\n","            self.data.append(self.transform(left))\n","\n","            z = x + y\n","            right = (\"{:*<6s}\".format(str(z))).replace(\"*\", \"<pad>\")\n","            right = self.transform(right, seq_len=5)\n","            right = [12] + right\n","            right[right.index(10)] = 12\n","            self.label.append(right)\n","        \n","\n","\n","        self.data = np.asarray(self.data)\n","        self.label = np.asarray(self.label)\n","\n","    def __getitem__(self, item):\n","        d = self.data[item]\n","        l = self.label[item]\n","        return d, l\n","\n","    def __len__(self):\n","        return self.data.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xTHnQ96hF7cd"},"source":["###エンコーダ・デコーダの作成\n","エンコーダとデコーダを用意します．エンコーダは，ワードエンベディングという特徴表現に変換する層とGRU層から構成されています．デコーダも同様の構造です．エンコーダ側の中間層の値がstateとして出力され，デコーダ側の中間層に入力されます．\n","エンコーダとデコーダは別々のネットワークとして用意し，それぞれの最適化にはAdamを利用します．"]},{"cell_type":"code","metadata":{"id":"_0IE4XYHF690"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","embedding_dim = 16\n","hidden_dim = 128\n","vocab_size = len(word2id)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class Encoder(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size=100):\n","        super(Encoder, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.batch_size = batch_size\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2id[\"<pad>\"])\n","        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n","\n","    def forward(self, indices):\n","        embedding = self.word_embeddings(indices)\n","        if embedding.dim() == 2:\n","            embedding = torch.unsqueeze(embedding, 1)\n","        _, state = self.gru(embedding, torch.zeros(1, self.batch_size, self.hidden_dim, device=device))\n","        \n","        return state\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size=100):\n","        super(Decoder, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.batch_size = batch_size\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=word2id[\"<pad>\"])\n","        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n","        self.output = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, index, state):\n","        embedding = self.word_embeddings(index)\n","        if embedding.dim() == 2:\n","            embedding = torch.unsqueeze(embedding, 1)\n","        gruout, state = self.gru(embedding, state)\n","        output = self.output(gruout)\n","        return output, state\n","\n","\n","encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size=100).to(device)\n","decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size=100).to(device)\n","criterion = nn.CrossEntropyLoss(ignore_index=word2id[\"<pad>\"])\n","\n","# Initialize opotimizers\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jPYDEx8F-d6"},"source":["###学習\n","学習を行います．学習データを2万サンプル生成して，データローダに与えます．\n","学習は200エポック行います．エンコーダの入力は数字または開始・終了・加算記号です．\n","デコーダの入力は計算結果です．\n","具体的には，54+37 を行う時，\n","エンコーダには，まず開始記号を最初に入力し，次に，5, 4, +, 3, 7 を入力します．そして，最後に終了記号を入力します．その時の中間層の情報をhidden_stateとしてエンコーダから受け取ります．\n","デコーダは，開始記号と中間情報(hidden_state)を最初に入力します，そして，計算結果の9, 1 を入力し，最後に終了記号を入力します．\n","この時，デコーダは各数字（または記号）の確率をdecoder_outputとして出力します．\n","decoder_outputは，[バッチサイズ, 1, 各クラス確率]の３次元なので，squeezeによって，[バッチサイズ,  各クラス確率] に次元削減します．\n","そして，クロスエントロピー誤差関数によって，ロスを求めます．\n","これを正解の長さ(=5)分繰り返し行い，ロスを累積します．\n","その後，誤差逆伝播，デコーダ，エンコーダの更新を行います．\n"]},{"cell_type":"code","metadata":{"id":"J6oQRPeTF-Bj"},"source":["import numpy as np\n","from time import time\n","\n","# GPUの確認\n","use_cuda = torch.cuda.is_available()\n","print('Use CUDA:', use_cuda)\n","\n","batch_size=100\n","epoch_num = 200\n","\n","train_data = CalcDataset(data_num = 20000)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","start = time()\n","for epoch in range(1, epoch_num+1):\n","    for data, label in train_loader:\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        if use_cuda:\n","            data = data.cuda()\n","            label = label.cuda()\n","\n","        encoder_hidden = encoder(data)\n","        source = label[:, :-1]\n","        target = label[:, 1:]\n","        decoder_hidden = encoder_hidden\n","\n","        loss = 0\n","        for i in range(source.size(1)):\n","            decoder_output, decoder_hidden = decoder(source[:, i], decoder_hidden)\n","            decoder_output = torch.squeeze(decoder_output)\n","            loss += criterion(decoder_output, target[:, i])\n","\n","        # Perform backpropagation\n","        loss.backward()\n","        \n","        # Adjust model weights\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","  \n","    elapsed_time = time() - start\n","    if epoch % 10 == 0:\n","        print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch, loss.item(), elapsed_time))\n","        \n","model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n","torch.save({\n","    'encoder_model': encoder.state_dict(),\n","    'decoder_model': decoder.state_dict(),\n","}, model_name)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVUbOtyfGLwA"},"source":["###評価\n","次に，学習したモデルを評価をします．テストデータを2000サンプル生成して，データローダに与えます．\n","ここで，学習時はエンコーダとデコーダのバッチサイズを100としていました．\n","テスト時は１つずつ行いたいので，エンコーダとデコーダを新たに生成し，学習したパラメータをロードします．\n","エンコーダ側に計算したい数字（または記号）を入力して中間情報stateを得ます．\n","デコーダ側に，中間情報stateと開始記号<eos>を入力します．\n","デコーダ側の出力は数字または記号(token)と中間情報です．\n","これらを繰り返しデコーダに入力します．<eos>が出力されたら繰り返しは終了です．\n","出力されたtokenを追加したリストrightを計算結果とします．\n","計算する式(left)を作成した後，evalでその計算結果が正しいかどうかを判定します．\n","\n"]},{"cell_type":"code","metadata":{"id":"OGdS8DWwGC_D"},"source":["\n","batch_size = 1\n","test_data = CalcDataset(data_num = 2000)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","encoder = Encoder(vocab_size, embedding_dim, hidden_dim, batch_size=1).to(device)\n","decoder = Decoder(vocab_size, embedding_dim, hidden_dim, batch_size=1).to(device)\n","\n","model_name = \"seq2seq_calculator_v{}.pt\".format(epoch)\n","checkpoint = torch.load(model_name)\n","encoder.load_state_dict(checkpoint[\"encoder_model\"])\n","decoder.load_state_dict(checkpoint[\"decoder_model\"])\n","\n","accuracy = 0\n","        \n","# 評価の実行   \n","with torch.no_grad():\n","    for data, label in test_loader:\n","        if use_cuda:\n","            data = data.cuda()\n","\n","        state = encoder(data)\n","\n","        right = []\n","        token = \"<eos>\"\n","        for _ in range(7):\n","            index = word2id[token]\n","            input_tensor = torch.tensor([index], device=device)\n","            output, state = decoder(input_tensor, state)\n","            prob = F.softmax(torch.squeeze(output))\n","            index = torch.argmax(prob.cpu().detach()).item()\n","            token = id2word[index]\n","            if token == \"<eos>\":\n","                break\n","            right.append(token)\n","        right = \"\".join(right)\n","        \n","        x = list(data[0].to('cpu').detach().numpy() )\n","        try:\n","            padded_idx_x = x.index(word2id[\"<pad>\"])\n","        except ValueError:\n","            padded_idx_x = len(x)\n","        left = \"\".join(map(lambda c: str(id2word[c]), x[:padded_idx_x]))\n","\n","\n","\n","        flag = [\"F\", \"T\"][eval(left) == int(right)]\n","        print(\"{:>7s} = {:>4s} :{}\".format(left, right, flag))\n","        if flag == \"T\":\n","            accuracy += 1\n","print(\"Accuracy: {:.2f}\".format(accuracy / len(test_loader)))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jVqKQdWxGOg5"},"source":["#課題\n","* 足し算だけでなく，色々な四則演算を試そう\n","* 他のリカレントニューラルネットワークを使って精度比較をしてみよう"]},{"cell_type":"code","metadata":{"id":"VMni_cMPGgJj"},"source":[""],"execution_count":null,"outputs":[]}]}