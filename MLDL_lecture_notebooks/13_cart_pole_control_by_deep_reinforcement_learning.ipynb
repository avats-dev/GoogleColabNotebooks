{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "13_cart_pole_control_by_deep_reinforcement_learning.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/GoogleColabNotebooks/blob/master/MLDL_lecture_notebooks/13_cart_pole_control_by_deep_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-aIYZaxHFn6r"
      },
      "source": [
        "# 13. 深層強化学習によるCart Pole制御\n",
        "\n",
        "---\n",
        "## 目的\n",
        "Deep Q Learningを用いてCart Pole制御を行う．\n",
        "ここで，Cart Pole制御とは台車に乗っている棒が倒れないように台車を左右に動かすことである．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5rQGfxWYK_4O"
      },
      "source": [
        "## 準備\n",
        "\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n",
        "\n",
        "\n",
        "### モジュールの追加インストール\n",
        "下記のプログラムを実行して，実験結果の表示に必要な追加ライブラリやモジュールをインストールする．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c0AFod2df5j5",
        "colab": {}
      },
      "source": [
        "!apt-get -qq -y install libcusparse9.1 libnvrtc9.1 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.9.1 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "\n",
        "!pip -q install gym\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CKpJ1mtDFoDo"
      },
      "source": [
        "## モジュールのインポート\n",
        "はじめに必要なモジュールをインポートする．\n",
        "\n",
        "今回はPyTorchに加えて，Cart Poleを実行するためのシミュレータであるopenAI Gym（gym）をインポートする．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c0oGGaN1FoH2",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# 使用するデバイス（GPU or CPU）の決定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Use device:\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Jo5u_p4SFoMa"
      },
      "source": [
        "## OpenAI GymによるCart Poleの環境の定義\n",
        " [OpenAI Gym](https://github.com/openai/gym) は，様々な種類の環境を提供しているモジュールです．\n",
        " \n",
        " 今回は古典的な制御問題であるCart Pole（倒立振子）を実行します．\n",
        " まず，gym.make関数で実行したい環境を指定します．\n",
        " その後，reset関数を実行することで，環境を初期化します．\n",
        " \n",
        "cart pole環境では，倒立振子の現在の状態を把握するために，振子の角度や台車の速度などの4次元の情報が与えられており，`observation_space`という変数で確認することができます．\n",
        "また，`action_space`という変数で，エージェントが取ることのできる行動の数を確認することができます．\n",
        "cart poleの場合は，台車を左右どちらかに移動させるという行動を取るため，行動の数は2となっています．\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DsZ__suDFoQd",
        "colab": {}
      },
      "source": [
        "# 環境の指定\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# 環境の初期化\n",
        "obs = env.reset()\n",
        "#env.render()\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)\n",
        "print('initial observation:', obs)\n",
        "\n",
        "# 行動の決定と決定した行動の入力\n",
        "action = env.action_space.sample()\n",
        "obs, r, done, info = env.step(action)\n",
        "print('next observation:', obs)\n",
        "print('reward:', r)\n",
        "print('done:', done)\n",
        "print('info:', info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3eAWsdGh7WOJ"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "\n",
        "ネットワークモデルを定義します．\n",
        "ここでは，環境からの情報を入力し，行動に対するQ値を出力するようなネットワークを定義するために，全結合層3層から構成されるネットワークとします．\n",
        "\n",
        "入力データのサイズをobs_size，出力する行動の数をn_actions，中間層のサイズをn_hiddenとし，ネットワークの作成時に変更できるようにしておきます．\n",
        "\n",
        "`forward`関数では，これまでに学習したニューラルネットワークネットワークの場合と同様の手順でデータの計算を行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xKXbqU8HeXlp",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, obs_size, n_actions, n_hidden = 50):\n",
        "        super(DQN, self).__init__()\n",
        "        self.obs_size = obs_size\n",
        "        self.n_actions = n_actions\n",
        "        self.l1 = nn.Linear(obs_size, n_hidden)\n",
        "        self.l2 = nn.Linear(n_hidden, n_hidden)\n",
        "        self.l3 = nn.Linear(n_hidden, n_actions)\n",
        "        self.act = nn.Tanh()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.act(self.l1(x))\n",
        "        h = self.act(self.l2(h))\n",
        "        h = self.l3(h)\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yLPEJNA3eXlr"
      },
      "source": [
        "## Replay memoryの作成\n",
        "\n",
        "DQNを含む強化学習では，Experience Replayと呼ばれる学習テクニックが頻繁に使用されます．\n",
        "Experience Replayでは，過去の経験をReplay memory (buffer) に蓄積したのち，蓄積した経験をランダムに選択し学習へ使用します．\n",
        "ここでは，replay memoryの定義を行います．\n",
        "\n",
        "memoryへは，現在の状態，その時に選択された行動，行動によって遷移した状態（次状態），その時の報酬の4種類の情報を1つの経験として蓄積します．\n",
        "まず，`Transition`という変数を定義します．\n",
        "ここでは，`state`, `action`, `next_state`, `reward`が1セットとなるようなデータ構造（辞書オブジェクト）を定義します．\n",
        "\n",
        "その後，ReplayMemoryクラスを定義します．\n",
        "ReplayMemoryクラスでは，memoryへ格納する経験の数（`capacity`），経験を蓄積するリスト（`memory`），蓄積した経験の数を示す（`position`）を定義します．\n",
        "`push`関数では，メモリへ経験を格納します．\n",
        "また，`sample`関数では，指定したバッチサイズ (`batch_size`) 分の経験をランダムに選択し，返す関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zuwdl1CBeXls",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnBKYLRdxss7",
        "colab_type": "text"
      },
      "source": [
        "## 実験パラメータおよび関数の定義\n",
        "\n",
        "次に，実験に使用するパラメータを定義します．\n",
        "`BATCH_SIZE`はミニバッチサイズ，`GAMMA`は割引率です．\n",
        "`EPS_START`は学習初期のepsilon-greedy法によりランダムな行動を選択させる割合，`EPS_END`は学習終了時の割合，`EPS_DECAY`は学習途中の変化率です\n",
        "`TARGET_UPDATE`は目標値を出力するtarget networkのパラメータを更新する頻度を示します．\n",
        "\n",
        "### select_action関数\n",
        "次に，行動を選択する関数`select_action`を定義します．\n",
        "この関数では，現在の環境，ネットワークモデルを引数とし，行動を決定する関数です．\n",
        "このとき，上記で指定したパラメータに従い，一定の割合でランダムに行動選択を行います．それ以外の場合は，使用しているネットワークへ環境情報を入力し，行動を決定します．\n",
        "\n",
        "\n",
        "### optimze_model関数\n",
        "また，学習を行うための`optimize_model`関数を定義します．\n",
        "ここでは，ReplayMemory，学習を行うネットワークモデル，目標値を決定するためのtarget_model，最適化手法のoptimzerオブジェクトを引数とします．\n",
        "まず，memoryに十分な数の経験が蓄積されていない場合は学習を行わないように定義します．\n",
        "\n",
        "十分な経験が蓄積されている場合，その中から指定したバッチサイズ分の経験をランダムに取得します．\n",
        "取得した経験から，状態，行動，報酬，次状態それぞれにデータを分けます．\n",
        "その後状態をネットワークへ入力し，行動価値関数を得ます．\n",
        "また，target_modelへ次状態を入力し，出力された価値関数と報酬から，目標値（`expected_state_action_value`）を算出します．\n",
        "算出した値を用いて，誤差を算出し，パラメータを更新します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bOkhmHY1eXlv",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "\n",
        "def select_action(state, model, current_steps):\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * current_steps / EPS_DECAY)\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return model(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(model.n_actions)]], device=device, dtype=torch.long)\n",
        "    \n",
        "    \n",
        "def optimize_model(memory, model, target_model, optimizer):\n",
        "    # 十分な数の経験が蓄積されているか確認\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # バッチサイズ分の経験を取得\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    \n",
        "    # 状態，行動，報酬等にデータを分ける\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "                                  device=device,\n",
        "                                  dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    \n",
        "    # 状態を入力して行動を取得（推論）\n",
        "    state_action_values = model(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # target_modelを用いた目標値の決定\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_model(non_final_next_states).max(1)[0].detach()\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # 誤差の算出\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # パラメータ更新\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in model.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc_lnDAFxss_",
        "colab_type": "text"
      },
      "source": [
        "## 学習\n",
        "学習を行います．\n",
        "\n",
        "学習回数（エピソード数）を200とします．\n",
        "\n",
        "次に，学習を行うネットワーク（`policy_net`）および目標値を算出するネットワーク（`target_net`）を生成します．\n",
        "この時，target_netのパラメータはpolicy_netと同じになるように，指定します．\n",
        "最適化手法として，RMSpropを使用します．\n",
        "さらに，Replay Memoryを生成します．\n",
        "\n",
        "学習を開始します．\n",
        "まず，環境を初期化し，経験をReplayMemoryへ蓄積します．\n",
        "十分に蓄積された後，optimize_model関数でパラメータの更新を行います．\n",
        "また，`TARGET_UPDATE`で指定した回数ごとに，target_netのパラメータをpolicy_netのパラメータと同じになるようにコピーを行います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8A7HnwRveXlx",
        "colab": {}
      },
      "source": [
        "num_episode = 200\n",
        "\n",
        "steps_done = 0\n",
        "episode_durations = []\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "policy_net= DQN(4, n_actions).to(device)\n",
        "target_net= DQN(4, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "# 学習の開始\n",
        "for i_episode in range(1, num_episode+1):\n",
        "    state = env.reset()\n",
        "    state = torch.from_numpy(state).view(-1, 4).to(device).float()\n",
        "    \n",
        "    for t in count():\n",
        "        action = select_action(state, policy_net, steps_done)\n",
        "        steps_done += 1\n",
        "        \n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        \n",
        "        if done:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.from_numpy(next_state).view(-1, 4).to(device).float()\n",
        "        \n",
        "        memory.push(state, action, next_state, reward)\n",
        "        state = next_state\n",
        "        \n",
        "        optimize_model(memory, policy_net, target_net, optimizer)\n",
        "        if done:\n",
        "            episode_durations.append(t+1)\n",
        "            break\n",
        "    \n",
        "    if i_episode % 10 == 0:\n",
        "        print(\"episode:\", i_episode, \"duration:\", t)\n",
        "\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "print(\"Complete\")\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksjONOyoxstC",
        "colab_type": "text"
      },
      "source": [
        "## 評価\n",
        "学習したネットワーク（エージェント）を確認してみます．\n",
        "\n",
        "ここでは，framesに描画したフレームを順次格納します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "14MSs0oPeXl2",
        "colab": {}
      },
      "source": [
        "# 結果を描画するための設定\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
        "\n",
        "\n",
        "frames = []\n",
        "for i in range(3):\n",
        "    state = env.reset()\n",
        "    state = torch.from_numpy(state).view(-1, 4).to(device).float()\n",
        "    done = False\n",
        "    R = 0\n",
        "    t = 0\n",
        "    \n",
        "    while not done and t < 200:\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "        action = policy_net(state)\n",
        "        state, r, done, _ = env.step(action.max(1)[1].view(1, 1).item())\n",
        "        state = torch.from_numpy(state).view(-1, 4).to(device).float()\n",
        "        R += r\n",
        "        t += 1\n",
        "    print(\"test episode:\", i, \"R:\", R)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5tJoQekxstF",
        "colab_type": "text"
      },
      "source": [
        "## 描画\n",
        "\n",
        "maptlotlibを用いて，保存した動画フレームをアニメーションとして作成し，表示しています．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wFDCxq1GeXl4",
        "colab": {}
      },
      "source": [
        "# 実行結果の表示\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}