{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"03_CIFAR_CNN.ipynb","provenance":[{"file_id":"https://github.com/machine-perception-robotics-group/GoogleColabNotebooks/blob/seminar1/MLDL_lecture_notebooks/03_CIFAR_CNN.ipynb","timestamp":1610954832007}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wJU2RPpSvlQT"},"source":["# 03. Object recognition of CIFAR10 dataset\n","\n","\n","---\n","## Purpose\n","Carry out object recognition of the CIFAR10 dataset. The structure of this program is similar to the MNIST character recognition program, so refer to that tutorial for a basic explanation. This page describes differences with the MNIST character recognition program.\n","\n","Compute neural network operations by using the GPU．\n"]},{"cell_type":"markdown","metadata":{"id":"5rQGfxWYK_4O"},"source":["## Preparations\n","\n","### Confirm and change Google Colaboratory settings\n","\n","In this tutorial, we use PyTorch to implement a neural network and carry out training and evaluation. **To process operations using the GPU, go to the menu bar at the top of screen and choose Runtime -> Change runtime type -> Hardware accelerator -> GPU.**\n"]},{"cell_type":"markdown","metadata":{"id":"C2tsYagqvloK"},"source":["## Dataset\n","\n","### CIFAR10 dataset\n","\n","We use the CIFAR10 dataset for object recognition in this tutorial. The CIFAR10 dataset is composed of images in 10 different classes representing airplanes, dogs, etc.\n","\n","![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)"]},{"cell_type":"markdown","metadata":{"id":"Xo4jjpmwvle1"},"source":["## Import modules\n","\n","First, import the necessary modules.\n","\n","### Confirm GPU settings\n","\n","Confirm computation using GPU is enabled.\n","If `Use CUDA: True` is displayed, it is possible to use the GPU to perform computation in PyTorch. **If Use CUDA: False is displayed, start from the procedures given in “Confirm and change Google Colaboratory settings” above and change the settings. Then import the modules again.**\n","\n"]},{"cell_type":"code","metadata":{"id":"iCeaCulfvlao"},"source":["# import modules\n","from time import time\n","import torch\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torchsummary\n","\n","# confirm GPU settings\n","use_cuda = torch.cuda.is_available()\n","print('Use CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ppjeW5MbysXC"},"source":["## Read and confirm dataset\n","\n","Load the training data (CIFAR10 dataset)．\n","\n","Confirm the size of the loaded data. The training data size is 50,000 images. The data dimensions of each image are 3x32x32. This indicates a color image of 32x32 pixels."]},{"cell_type":"code","metadata":{"id":"K_xx-TkVvls6"},"source":["train_data = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=transforms.ToTensor(), download=True)\n","test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False, transform=transforms.ToTensor(), download=True)\n","\n","print(train_data)\n","print(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LKi4gTk8vlxe"},"source":["### Display CIFAR10 dataset\n","\n","Display the images in the CIFAR10 dataset. Here we use a program that uses matplotlib to display multiple images.\n"]},{"cell_type":"code","metadata":{"id":"sI33R2gVvl2P"},"source":["import matplotlib.pyplot as plt\n","\n","cols = 10\n","rows = 2\n","\n","plt.clf()\n","fig = plt.figure(figsize=(14, 4.8))\n","for r in range(rows):\n","    for c in range(cols):\n","        ax = fig.add_subplot(r+1, cols, c+1)\n","        ax.imshow(train_data[c+r*cols][0].permute(1, 2, 0))\n","        ax.set_axis_off()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgDd3iX2zmSV"},"source":["## Define neural network model\n","\n","Define the convolutional neural network.\n","\n","The network in this tutorial consists of two convolutional layers and three fully connected layers.\n","\n","The first convolutional layer has 1 input channel, 16 output feature maps, and a 3x3 convolution filter. The second convolutional layer has 16 input channels, 32 output feature maps, and convolution filter that also has a size of 3x3. The first fully connected layer has `7*7*32` input units and 1024 output units. The next fully connected layer has 1024 input units and 1024 output units. The output layer has 1024 input units and 10 output units. For the activation function, we define a sigmoid function in `self.act`. In addition, we define self.pool to carry out pooling. For this example, we use max pooling. We define the composition of each layer using the `__init__` function.\n","\n","\n","The `forward` function describes how to connect and process the defined layers. The `forward` function’s parameter `x` represents the input data. This parameter’s argument is inputted to `conv1` defined by the `__init__` function. The output is passed to the activation function `self.act`. The output of that function is passed to `self.pool`. The result of pooling is outputted as `h`. The second convolutional layer is also processed using the same procedures.\n","\n","After convolution is applied to the feature map, the map is inputted to the fully connected layers. Identification results are outputted. First, the shape (channel x height x width) of the feature map obtained by convolution is converted to a one-dimensional array. Here we manipulate array `h` by using `view()`. We obtain the first dimension of the size of h with the first argument, `h.size()[0]`, and specify it as the size of the first dimension of the array after conversion. The second argument, `-1`, specifies an arbitrary size. Doing so transforms `h` to the shape (number of batches x arbitrary length of data). Finally, the class scores are returned by sequentially inputting the converted `h` to the fully connected layers and the activation function.\n"]},{"cell_type":"code","metadata":{"id":"TNHnp_YczmY3"},"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","        self.l1 = nn.Linear(8 * 8 * 32, 1024)\n","        self.l2 = nn.Linear(1024, 1024)\n","        self.l3 = nn.Linear(1024, 10)\n","        self.act = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","    \n","    def forward(self, x):\n","        h = self.pool(self.act(self.conv1(x)))\n","        h = self.pool(self.act(self.conv2(h)))\n","        h = h.view(h.size()[0], -1)\n","        h = self.act(self.l1(h))\n","        h = self.act(self.l2(h))\n","        h = self.l3(h)\n","        return h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Dwuvfouzmd7"},"source":["## Create neural network\n","\n","\n","Create the neural network defined by the program above.\n","\n","Call the `CNN` class to define the neural network model. If using the GPU （`use_cuda == True`）, the network model is placed in GPU memory. This makes it possible to perform operations using the GPU.\n","We use stochastic gradient descent with momentum (SGD with momentum) as the optimization technique when training. We pass 0.01 as the argument of the learning rate parameter and 0.9 as the argument of the momentum parameter.\n","\n","\n","Finally, `torchsummary.summary()` is used to display detailed information about the defined network.\n"]},{"cell_type":"code","metadata":{"id":"23m79Eq-zmjl"},"source":["model = CNN()\n","if use_cuda:\n","    model.cuda()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# display the detailed information about the defined network\n","torchsummary.summary(model, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MUNa9Xe79vAG"},"source":["## Training\n","Carry out training by using the loaded CIFAR10 dataset and the created neural network.\n","\n","We set the data size for calculating errors for one pass (mini-batch size) as 64 and the number of training epochs as 10. \n","\n","Next, we define the data loader. The data loader uses the training dataset (`train_data`) that was loaded above and creates an object that reads the data in the mini-batch size as specified by the assignment statement below. For this training, we set `shuffle=True` to specify that the data is to be read randomly each time.\n","\n","Next, we set the error function. Because we are dealing with a classification problem here, we define `criterion` to be `CrossEntropyLoss` to calculate cross entropy error.\n","\n","Begin training.\n","\n","For each update, the data to be learned and the teacher data are given the names `image` and `label`, respectively. The training model is given an image and obtains the probability y for each class. The error between each class’s probability y and the teacher label is calculated by `criterion`. The recognition accuracy is also calculated. The error is then backpropagated by the backward function to update the neural network.\n"]},{"cell_type":"code","metadata":{"id":"68RE3RTa76-W"},"source":["# set the mini-batch size and training epocs\n","batch_size = 64\n","epoch_num = 10\n","n_iter = len(train_data) / batch_size\n","\n","# define data loader\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","# set the error (loss) function\n","criterion = nn.CrossEntropyLoss()\n","if use_cuda:\n","    criterion.cuda()\n","\n","# switch the network configuration into training mode\n","model.train()\n","\n","start = time()\n","for epoch in range(1, epoch_num+1):\n","    sum_loss = 0.0\n","    count = 0\n","    \n","    for image, label in train_loader:\n","        \n","        if use_cuda:\n","            image = image.cuda()\n","            label = label.cuda()\n","            \n","        y = model(image)\n","        \n","        loss = criterion(y, label)\n","        \n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        sum_loss += loss.item()\n","        \n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","        \n","    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n","                                                                                 sum_loss / n_iter,\n","                                                                                 count.item() / len(train_loader),\n","                                                                                 time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"119eIrSmzmw6"},"source":["## Testing\n","Evaluate by using the trained network model on the testing data."]},{"cell_type":"code","metadata":{"id":"yoYVMRGLzm1I"},"source":["# データローダーの準備\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n","\n","# ネットワークを評価モードへ変更\n","model.eval()\n","\n","# 評価の実行\n","count = 0\n","with torch.no_grad():\n","    for image, label in test_loader:\n","\n","        if use_cuda:\n","            image = image.cuda()\n","            label = label.cuda()\n","            \n","        y = model(image)\n","\n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","\n","print(\"test accuracy: {}\".format(count.item() / 10000.))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gzl4N5rC4j5u"},"source":["## Problems\n","\n","### 1. Change the neural network structure and confirm the change in recognition accuracy.\n","\n","**Hint: The following items can change the neural network structure.**\n","* The number of units in intermediate layers, convolution kernel size of convolution\n","* Number of layers\n","* The activation function\n","  * For example, `nn.Tanh()` or `nn.ReLU()`, `nn.LeakyReLU()`, etc.\n","  * Other activation functions that can be used in PyTorch are summarized on [this page](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n","\n","\n","### 2. Change training settings and confirm the change in recognition accuracy.\n","\n","**Hint: The following settings that can be changed in the program**\n","* Mini-batch size\n","* Number of training cycles (number of epochs)\n","* Learning rate\n","* Optimization method\n","  * Choices include `torch.optim.Adagrad()` and `torch.optim.Adam()`．\n","  * Optimization methods that can be used in PyTorch are summarized on [this page](https://pytorch.org/docs/stable/optim.html#algorithms).\n"]}]}