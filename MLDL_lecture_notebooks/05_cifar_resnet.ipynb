{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"05_cifar_resnet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wJU2RPpSvlQT"},"source":["# 05. Object recognition of CIFAR10 dataset using ResNet\n","\n","\n","---\n","## Purpose\n","\n","Carry out object recognition of the CIFAR10 dataset. The structure of this program is similar to the MNIST character recognition program, so refer to that tutorial for a basic explanation. This page describes differences with the MNIST character recognition program.\n","\n","Compute neural network operations by using the GPU Also, confirm the effect data augmentation on learning. In this tutorial, we use a residual neural network (ResNet) as the convolutional neural network model."]},{"cell_type":"markdown","metadata":{"id":"5rQGfxWYK_4O"},"source":["## Preparations\n","\n","### Confirm and change Google Colaboratory settings\n","\n","In this tutorial, we use PyTorch to implement a neural network and carry out training and evaluation. **To process operations using the GPU, go to the menu bar at the top of screen and choose Runtime -> Change runtime type -> Hardware accelerator -> GPU.**"]},{"cell_type":"markdown","metadata":{"id":"C2tsYagqvloK"},"source":["## Dataset\n","\n","### CIFAR10 dataset\n","\n","We use the CIFAR10 dataset for object recognition in this tutorial. The CIFAR10 dataset is composed of images in 10 different classes representing airplanes, dogs, etc.\n","\n","![CIFAR10_sample.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/176458/b6b43478-c85f-9211-7bc6-227d9b387af5.png)"]},{"cell_type":"markdown","metadata":{"id":"Xo4jjpmwvle1"},"source":["## Import modules\n","\n","\n","First, import the necessary modules.\n","\n","\n","### Confirm GPU settings\n","\n","Confirm computation using GPU is enabled.\n","\n","If `Use CUDA: True` is displayed, it is possible to use the GPU to perform computation in PyTorch. If Use CUDA: False is displayed, start from the procedures given in “Confirm and change Google Colaboratory settings” above and change the settings. Then import the modules again.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"iCeaCulfvlao"},"source":["# import modules\n","from time import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torchsummary\n","\n","# confirm GPU settings\n","use_cuda = torch.cuda.is_available()\n","print('Use CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ppjeW5MbysXC"},"source":["## Read and confirm dataset\n","Load the training data (CIFAR10 dataset)．\n","\n"]},{"cell_type":"code","metadata":{"id":"K_xx-TkVvls6"},"source":["transform_train = transforms.Compose([transforms.RandomCrop(32, padding=1),\n","                                      transforms.RandomHorizontalFlip(),\n","                                      transforms.ToTensor()])\n","transform_test = transforms.Compose([transforms.ToTensor()])\n","\n","train_data = torchvision.datasets.CIFAR10(root=\"./\", train=True, transform=transform_train, download=True)\n","test_data = torchvision.datasets.CIFAR10(root=\"./\", train=False, transform=transform_test, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgDd3iX2zmSV"},"source":["## Define neural network\n","Define the Residual Network (ResNet)．\n","\n","A ResNet is composed of structures called bottlenecks. First, we use `BottleNeck(nn.Module)` to create a class that can define a bottleneck in an arbitrary form.\n","`in_planes`, a parameter of the `__init__` function, specifies the number of input feature map channels. `planes` specifies the number of feature map channels in a bottle neck. \n","\n","The function `nn.Sequential()` is used in `__init__` to define the layers. It receives a list containing multiple layers as an argument and defines an object bringing together these layers (container of layers). In the function below, convolution and batch normalization are performed within the list. When `self.convs`, defined by `nn.Sequential`, is operated on—in short, when `self.convs(x)` is used in the function `forward()`—arguments in the list are operated on and returned sequentially.\n","\n","We define a `ResNet` (here, ResNet50) by using the bottleneck structure defined above. `self._make_layer()`, which is defined in a ResNet cluster, defines Residual Block (layer composed of multiple bottlenecks) of arbitrary form. A Residual Block specifies the number of channels `planes`, the number of bottlenecks `num_blocks` and the stride of the convolution `stride`.\n","Next, in accordance with these arguments, bottlenecks specified by quantity and parameters are stored in a list. Finally, using `nn.Sequential` as explained above, a block of layers is defined and returned to define a Residual Block with an arbitrary number of layers. \n","\n","\n","Using `_make_layer()`, the entire ResNet is defined in `__init__`．Adaptive average Pooling (`AdaptiveAvgPool2d()`) applies average pooling to a feature map of arbitrary size.\n","The arguments `(1, 1)` specify performing average pooling so that an input feature map of any size will become a 1x1 feature map.   \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"TNHnp_YczmY3"},"source":["class BottleNeck(nn.Module):\n","    expansion = 4\n","    def __init__(self, in_planes, planes, stride=1):\n","        super().__init__()\n","        self.convs = nn.Sequential(*[nn.Conv2d(in_planes, planes, kernel_size=1, bias=False),\n","                                     nn.BatchNorm2d(planes),\n","                                     nn.ReLU(inplace=True),\n","                                     nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False),\n","                                     nn.BatchNorm2d(planes),\n","                                     nn.ReLU(inplace=True),\n","                                     nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False),\n","                                     nn.BatchNorm2d(self.expansion * planes)])\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        out = self.convs(x)\n","        out += self.shortcut(x)\n","        out = self.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, n_class=10, n_blocks=[3, 4, 6, 3]):\n","        super().__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU()\n","        \n","        self.res1 = self._make_layer(64, n_blocks[0], stride=1)\n","        self.res2 = self._make_layer(128, n_blocks[1], stride=2)\n","        self.res3 = self._make_layer(256, n_blocks[2], stride=2)\n","        self.res4 = self._make_layer(512, n_blocks[3], stride=2)\n","        \n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(2048, n_class)\n","\n","    def _make_layer(self, planes, num_blocks, stride):\n","        strides = [stride] + [1] * (num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(BottleNeck(self.in_planes, planes, stride))\n","            self.in_planes = planes * BottleNeck.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        h = self.relu(self.bn1(self.conv1(x)))\n","        h = self.res1(h)\n","        h = self.res2(h)\n","        h = self.res3(h)\n","        h = self.res4(h)\n","        h = self.avgpool(h)\n","        h = torch.flatten(h, 1)\n","        h = self.fc(h)\n","        return h\n","        \n","        \n","class ResNet50(ResNet):\n","    def __init__(self, n_class=10):\n","        super(ResNet50, self).__init__(n_class, n_blocks=[3, 4, 6, 3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Dwuvfouzmd7"},"source":["## Create neural network\n","\n","Create the neural network defined by the program above.\n","\n","Call the CNN class to define the neural network model. If using the GPU （`use_cuda == True`）, the network model is placed in GPU memory. This makes it possible to perform operations using the GPU.\n","\n","We use stochastic gradient descent with momentum (SGD with momentum) as the optimization technique when training. We pass 0.01 as the argument of the learning rate parameter and 0.9 as the argument of the momentum parameter.\n","\n","Finally, `torchsummary.summary()` is used to display detailed information about the defined network. "]},{"cell_type":"code","metadata":{"id":"23m79Eq-zmjl"},"source":["model = ResNet50(n_class=10)\n","if use_cuda:\n","    model.cuda()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# display detailed information about the defined network\n","torchsummary.summary(model, (3, 32, 32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MUNa9Xe79vAG"},"source":["## Training\n","We set the data size for calculating errors for one pass (mini-batch size) as 128 and the number of training epochs as 100. We get the number of updates in one epoch by obtaining the size of CIFAR10 training data. The training model is given image and obtains the probability y for each class. The error between each class’s probability y and the teacher label is calculated by the softmax cross entropy error function. The recognition accuracy is also calculated. The error is then backpropagated by the backward function to update the neural network. "]},{"cell_type":"code","metadata":{"id":"68RE3RTa76-W"},"source":["# set mini-batch size and training epochs\n","batch_size = 128\n","epoch_num = 10\n","n_iter = len(train_data) / batch_size\n","\n","# define data loader\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# set error (loss) function\n","criterion = nn.CrossEntropyLoss()\n","if use_cuda:\n","    criterion.cuda()\n","\n","# switch training mode\n","model.train()\n","\n","start = time()\n","for epoch in range(1, epoch_num+1):\n","    sum_loss = 0.0\n","    count = 0\n","    \n","    for image, label in train_loader:\n","        if use_cuda:\n","            image = image.cuda()\n","            label = label.cuda()\n","\n","        y = model(image)\n","        loss = criterion(y, label)\n","        \n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        sum_loss += loss.item()\n","        \n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","\n","    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n","                                                                                 sum_loss / n_iter,\n","                                                                                 count.item() / len(train_loader),\n","                                                                                 time() - start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"119eIrSmzmw6"},"source":["## Testing\n","\n","Evaluate by using the trained network model on the testing data."]},{"cell_type":"code","metadata":{"id":"yoYVMRGLzm1I"},"source":["# define data loader\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n","\n","# switch evaluation mode\n","model.eval()\n","\n","# begin evaluatin\n","count = 0\n","with torch.no_grad():\n","    for image, label in test_loader:\n","\n","        if use_cuda:\n","            image = image.cuda()\n","            label = label.cuda()\n","            \n","        y = model(image)\n","\n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","\n","print(\"test accuracy: {}\".format(count.item() / 10000.))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_U8wsW37hUUI"},"source":["## 課題\n","\n","\n","### 1. Change training settings and confirm the change in recognition accuracy.\n","\n","**Hint: The following settings that can be changed in the program**\n","* Mini-batch size\n","* Number of training cycles (number of epochs)\n","* Learning rate\n","* Optimization method\n","  * Choices include `torch.optim.Adagrad()` and `torch.optim.Adam()`．\n","  * Optimization methods that can be used in PyTorch are summarized on [this page](https://pytorch.org/docs/stable/optim.html#algorithms).\n","\n","\n","### 2. Add types of data augmentation and carry out training.\n","\n","**Hint: You can change the data augmentation used for training in transform_train.**\n","\n","```python\n","transform_train = transforms.Compose([(Add augmentation you wish to use here) ,\n","                                      transforms.ToTensor()])\n","```\n","\n","Data augmentations you can use in PyTorch (torchvision) are summarized on [this page](https://pytorch.org/docs/stable/torchvision/transforms.html).\n","\n"]}]}