{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"02_MNIST_CNN.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xP6-w6Uxb6jR"},"source":["# 02. Character recognition of MNIST dataset using CNN\n","\n","\n","---\n","## Purpose\n","\n","Carry out character recognition of the MNIST dataset using a Convolutional Neural Network (CNN). For evaluation, calculate the recognition rate of each class using a confusion matrix.\n","\n","Also, compute neural network operations by using the GPU."]},{"cell_type":"markdown","metadata":{"id":"5rQGfxWYK_4O"},"source":["## Preparations\n","\n","### Confirm and change Google Colaboratory settings\n","\n","In this tutorial, we use PyTorch to implement a neural network and carry out training and evaluation.\n","**To process operations using the GPU, go to the menu bar at the top of screen and choose Runtime -> Change runtime type -> Hardware accelerator -> GPU.** "]},{"cell_type":"markdown","metadata":{"id":"RsGSLNkYQmkG"},"source":["## Import modules\n","\n","First, import the necessary modules."]},{"cell_type":"code","metadata":{"id":"SLeGt2xaNFOB"},"source":["from time import time\n","import torch\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import torchsummary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FjrYHYpuLbrx"},"source":["### Confirm GPU settings\n","\n","Confirm computation using GPU is enabled.\n","\n","\n","If `Use CUDA: True` is displayed, it is possible to use the GPU to perform computation in PyTorch. If Use CUDA: False is displayed, start from the procedures given in “Confirm and change Google Colaboratory settings” above and change the settings. Then import the modules again.\n"]},{"cell_type":"code","metadata":{"id":"6wYHKJ-WLbry"},"source":["use_cuda = torch.cuda.is_available()\n","print('Use CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ue60y-upamyo"},"source":["## Read and confirm dataset\n","\n","Load the training data (MNIST dataset)．"]},{"cell_type":"code","metadata":{"id":"n7zpMk-4axYm"},"source":["train_data = torchvision.datasets.MNIST(root=\"./\", train=True, transform=transforms.ToTensor(), download=True)\n","test_data = torchvision.datasets.MNIST(root=\"./\", train=False, transform=transforms.ToTensor(), download=True)\n","\n","print(type(train_data.data), type(train_data.targets))\n","print(type(test_data.data), type(test_data.targets))\n","print(train_data.data.size(), train_data.targets.size())\n","print(test_data.data.size(), test_data.targets.size())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G418kZOgToXR"},"source":["## Define network model\n","\n","Define the convolutional neural network.\n","\n","\n","The network in this tutorial consists of two convolutional layers and three fully connected layers.\n","\n","The first convolutional layer has 1 input channel, 16 output feature maps, and a 3x3 convolution filter. The second convolutional layer has 16 input channels, 32 output feature maps, and convolution filter that also has a size of 3x3. The first fully connected layer has `7*7*32` input units and 1024 output units. The next fully connected layer has 1024 input units and 1024 output units. The output layer has 1024 input units and 10 output units. For the activation function, we define a sigmoid function in `self.act`. In addition, we define `self.pool` to carry out pooling. For this example, we use max pooling. We define the composition of each layer using the `__init__` function.\n","\n","\n","The `forward` function describes how to connect and process the defined layers. The `forward` function’s parameter `x` represents the input data.\n","This parameter’s argument is inputted to `conv1` defined by the `__init__` function.\n","The output is passed to the activation function `self.act`.\n","The output of that function is passed to `self.pool`. The result of pooling is outputted as `h`.\n","The second convolutional layer is also processed using the same procedures.\n","\n","\n","After convolution is applied to the feature map, the map is inputted to the fully connected layers. \n","Identification results are outputted. First, the shape (channel x height x width) of the feature map obtained by convolution is converted to a one-dimensional array. Here we manipulate array h by using `view()`. We obtain the first dimension of the size of h with the first argument, `h.size()[0]`, and specify it as the size of the first dimension of the array after conversion. The second argument, `-1`, specifies an arbitrary size. Doing so transforms `h` to the shape (number of batches `x` arbitrary length of data). Finally, the class scores are returned by sequentially inputting the converted `h` to the fully connected layers and the activation function.\n"]},{"cell_type":"code","metadata":{"id":"8FJhkBJnTuPd"},"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","        self.l1 = nn.Linear(7*7*32, 1024)\n","        self.l2 = nn.Linear(1024, 1024)\n","        self.l3 = nn.Linear(1024, 10)\n","        self.act = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","    def forward(self, x):\n","        h = self.pool(self.act(self.conv1(x)))\n","        h = self.pool(self.act(self.conv2(h)))\n","        h = h.view(h.size()[0], -1)\n","        h = self.act(self.l1(h))\n","        h = self.act(self.l2(h))\n","        h = self.l3(h)\n","        return h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ijVjOGVhb6vs"},"source":["## Create neural network\n","\n","Create the neural network defined by the program above.\n","\n","Call the `CNN` class to define the neural network model. If using the GPU （`use_cuda == True`）, the network model is placed in GPU memory. This makes it possible to perform operations using the GPU.\n","\n","We use stochastic gradient descent with momentum (SGD with momentum) as the optimization technique when training. We pass 0.01 as the argument of the learning rate parameter and 0.9 as the argument of the momentum parameter.\n","\n","Finally, `torchsummary.summary()` is used to display detailed information about the defined network. \n","\n"]},{"cell_type":"code","metadata":{"id":"SyfYfpXvb62g"},"source":["model = CNN()\n","if use_cuda:\n","    model.cuda()\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# display the detailed information about the defined network\n","if use_cuda:\n","    torchsummary.summary(model, (1, 28, 28), device='cuda')\n","else:\n","    torchsummary.summary(model, (1, 28, 28), device='cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhbw4THgb680"},"source":["## Training\n","\n","Carry out training by using the loaded MNIST dataset and created neural network.\n","\n","We set the number of data for calculating errors for one pass (mini-batch size) as 100 and the number of training epochs as 10. \n","\n","Next, we define the data loader. The data loader uses the training dataset (`train_data`) that was loaded above and creates an object that reads the data in the mini-batch size as specified by the assignment statement below. For this training, we set `shuffle=True` to specify that the data is to be read randomly each time.\n","\n","Next, we set the error function. Because we are dealing with a classification problem here, we define `criterion` to be `CrossEntropyLoss` to calculate cross entropy error.\n","\n","Begin training.\n","\n","For each update, the data to be learned and the teacher data are given the names `image` and `label`, respectively. The training model is given an image and obtains the probability y for each class. The error between each class’s probability y and the teacher label is calculated by `criterion`. The recognition accuracy is also calculated. The error is then backpropagated by the backward function to update the neural network. \n","\n"]},{"cell_type":"code","metadata":{"id":"UsBaxg2Wb7Dp"},"source":["# set the mini-batch size and training epochs\n","batch_size = 100\n","epoch_num = 10\n","\n","# define data loader\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","# set the error (loss) function\n","criterion = nn.CrossEntropyLoss()\n","if use_cuda:\n","    criterion.cuda()\n","\n","# swich the network configuration into the training mode\n","model.train()\n","\n","# begin training\n","train_start = time()\n","for epoch in range(1, epoch_num+1):\n","    sum_loss = 0.0\n","    count = 0\n","\n","    for image, label in train_loader:\n","\n","        if use_cuda:\n","            image = image.cuda()\n","            label = label.cuda()\n","\n","        y = model(image)\n","\n","        loss = criterion(y, label)\n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        sum_loss += loss.item()\n","\n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","\n","    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed time: {}\".format(epoch, sum_loss/600, count.item()/60000., time() - train_start))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f5oxc_C-b6g9"},"source":["## Testing\n","\n","Evaluate by using the trained network model on the testing data. \n","\n","Apply `model.eval()` to change network operations to evaluation mode. This enables different operations (e.g. dropout) to behave differently in evaluation mode instead of training mode.\n","Apply `torch.no_grad()` to carry out operations without keeping gradient information that is required during training.\n"]},{"cell_type":"code","metadata":{"id":"eDwQ-iJtjSaL"},"source":["# defnine data loader\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=False)\n","\n","# switch the network configuration into evaluation mode\n","model.eval()\n","\n","# begin evaluation\n","count = 0\n","with torch.no_grad():\n","    for image, label in test_loader:\n","\n","        if use_cuda:\n","            image = image.cuda()\n","            label = label.cuda()\n","            \n","        y = model(image)\n","\n","        pred = torch.argmax(y, dim=1)\n","        count += torch.sum(pred == label)\n","\n","print(\"test accuracy: {}\".format(count.item() / 10000.))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RO9gksBuj0qm"},"source":["## Problems\n","\n","\n","### 1. Confirm the difference in computation time when training using GPU compared with using CPU.\n","\n","**Hint: You can switch between GPU and CPU by changing the value of `use_cuda` (`True` or `False`) in the \"GPU Confirmation\" cell (at the top of this page).**\n","\n","\n","\n","### 2. Change the neural network structure and confirm the change in recognition accuracy. \n","\n","**Hint: The following items can change the neural network structure. **\n","\n","* The number of units in intermediate layers\n","* Number of layers\n","* The activation function\n","  * For example, `nn.Tanh()` or `nn.ReLU()`, `nn.LeakyReLU()`, etc.\n","  * Other activation functions that can be used in PyTorch are summarized on [this page](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n","\n","\\* After changing the neural network structure, use the function `torchsummary.summary()` to view changes in the number of parameters.\n","\n","\n","### 3. hange training settings and confirm the change in recognition accuracy.\n","\n","**Hint: The following settings that can be changed in the program**\n","* Mini-batch size\n","* Number of training cycles (number of epochs)\n","* Learning rate\n","* Optimization method\n","  * Choices include `torch.optim.Adagrad()` and `torch.optim.Adam()`.\n","  * Optimization methods that can be used in PyTorch are summarized on [this page](https://pytorch.org/docs/stable/optim.html#algorithms).\n","\n"]}]}